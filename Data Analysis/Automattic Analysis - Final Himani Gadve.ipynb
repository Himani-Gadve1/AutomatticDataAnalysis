{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This jupyter notebook contains an analysis on the public blog post. There are about 1.8 million total posts in this data set. All of these data are publicly available on the web. The data has the following format:\n",
    "\n",
    "{\n",
    "    “blog_id”: id identifying the blog\n",
    "    \n",
    "    “post_id”: id identifying the post on that blog\n",
    "    \n",
    "    “lang”: the language code for the post (this is a combination of the user setting and our  language detection algorithm\n",
    "    \n",
    "    “url”: url to the post (but without ‘http(s)://’)\n",
    "    \n",
    "    “date_gmt”: date and time that the post was published\n",
    "    (as set by the user). example: “2010-01-30 14:48:55\"\n",
    "    \n",
    "    “title”: text title of the post\n",
    "    \n",
    "    “content”: text of the post with all html stripped out\n",
    "    \n",
    "    “author”: authors displayed name\n",
    "    \n",
    "    “author_login”: authors login username\n",
    "    \n",
    "    “author_id”: user id of the author\n",
    "    \n",
    "    “liker_ids”: array of user ids for people who have liked this post\n",
    "    \n",
    "    “like_count”: number of likes for this post\n",
    "    \n",
    "    “commenter_ids”: array of user ids for people who have commented on this post\n",
    "    \n",
    "    “comment_count”: number of comments on this post\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tableau visualization link: \n",
    "https://public.tableau.com/app/profile/himani.gadve/viz/Automattic_userengagmentanalysis/Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the required packages\n",
    "import os\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# dataframe and series \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta, date\n",
    "from datetime import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "#Setting Warnings Settings \n",
    "warnings.filterwarnings(action='once') \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x) #display changed from scientific to numeric format\n",
    "pd.set_option('expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON to CSV Conversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/himani/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# getting data from json.gz file\n",
    "#%%\n",
    "df_data = []\n",
    "with gzip.open('posts.jsonl.gz') as data:\n",
    "    for i in data:\n",
    "        df_data.append(json.loads(i.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1809199\n",
      "{'comment_count': 0, 'content': \"The Snap! Jamaal Jackson tore knee ligaments in their win over the Denver Broncos. Making his first NFL start at center will be Nick Cole. Cole filled in at center against the Broncos, but McNabb fumbled two snaps, and three offensive linemen were penalized for false starts. While you can&#39;t blame all the false starts on the new center, it certainly doesn&#39;t make things easier for the linemen. \\xa0 The good news is that Brian Westbrook is back after missing 5 games. His performance last week against the Broncos was promising, he had 9 carries for a total of 32 yards. In his last game against Dallas, Westbrook carried the ball 13 times for 50 yards rushing. \\xa0 Does it matter that Michael Vick may not be available to play? Probably not, but we&#39;d certainly like to see him out on the field for a few key plays.\\xa0If McNabb gets hurt, Kevin Kolb\\xa0who proved himself against the Chiefs\\xa0can help the Eagles soar to victory. \\xa0 Bring out your big guns Dallas, we&#39;re flying into town with victory on our minds!  Posted via email   from Jason's posterous  \", 'author': 'jasontromm', 'title': 'Biggest Challenge for the Eagles This Week?', 'like_count': 0, 'author_login': 'jasontromm', 'blog_id': 753, 'date_gmt': '2009-12-31 16:27:39', 'author_id': 762, 'post_id': 969, 'lang': 'en', 'url': 'jasontromm.wordpress.com/2009/12/31/biggest-challenge-for-the-eagles-this-week'}\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "# to see the length of the data, it means total number of likes and comments also\n",
    "print(len(df_data))\n",
    "\n",
    "# to see the first row of the list\n",
    "print(df_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "df = pd.DataFrame.from_dict(df_data) # convert dictionary to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "df.to_csv('blog_post.csv', index = False) # to use easily everytime, I write it to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "pd.options.display.max_columns=100 # To see the hidden columns in dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "Let’s have a look at data dimensionality, feature names, and feature types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f1c738de02b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#%%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'blog_post.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date_gmt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#getting main data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "df = pd.read_csv('blog_post.csv', parse_dates=['date_gmt'], low_memory=False) #getting main data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# From the output, we can see that the table contains 1973758 rows and 14 columns.\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for missing values in key dimensions: 'blog_id','author_id','post_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# making new data frame with dropped NA values\n",
    "df1 = df.dropna(axis = 0, subset = ['blog_id','author_id','post_id'], how = 'all')\n",
    "  \n",
    "# comparing sizes of data frames\n",
    "print(\"Old data frame length:\", len(df), \"\\nNew data frame length:\", \n",
    "       len(df1), \"\\nNumber of rows with at least 1 NA value: \",\n",
    "       (len(df)-len(df1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# droppping all the records with null values for 'blog_id','author_id','post_id'\n",
    "df.dropna(axis = 0, subset = ['blog_id','author_id','post_id'], how = 'all',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "# converting date from string to datetime and dropping dates which are not in the correct formate\n",
    "df['date_gmt'] = pd.to_datetime(df['date_gmt'].str[:10],  infer_datetime_format=True, errors = 'coerce')\n",
    "# drop null dates from analysis\n",
    "df.dropna(axis = 0, subset = ['date_gmt'], how = 'all',inplace = True)\n",
    "print(df['date_gmt'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "# pd.to_datetime(df['date_gmt'].str[:10],  infer_datetime_format=True)\n",
    "# df['date_gmt'] = pd.to_datetime(df['date_gmt'],format='%d/%m/%Y', errors = 'coerce')\n",
    "df['comment_count'] = pd.to_numeric(df['comment_count'], errors='coerce')\n",
    "df[\"comment_count\"] = pd.to_numeric(df[\"comment_count\"], downcast=\"float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After converting a missing value, data looks good for key dimesions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the median and the mean numbers of likes per post in this data sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "df_meadian=df.groupby(['post_id']).agg({'like_count':np.median}).reset_index()\n",
    "df_meadian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "df_mean=df.groupby(['post_id']).agg({'like_count':np.mean}).reset_index()\n",
    "df_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings:\n",
    "\n",
    "By comparing the mean and median of likes count per post for the first 5 post, median value looks to be zero and mean value looks like 1 to 6 likes per post in first five post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the total daily count of likes vs total daily comments for this data sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the Daily count of likes vs comments, we need to first understand the timeseries data for all the years which can be possible by dividing analysis in 3 phases;\n",
    "- Yearly Trends\n",
    "- Monthly Trends\n",
    "- Daily Trends based on the cohort filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# converting object to datetime\n",
    "df['date_gmt'] =  pd.to_datetime(df['date_gmt'],  infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# total daily count of likes vs total daily comments \n",
    "df1 = df.groupby(df.date_gmt.dt.date).agg({'like_count':'sum','comment_count':'sum'}).reset_index()\n",
    "df1.sort_values('date_gmt',ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yearly trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "df['Year_gmt'] = pd.to_datetime(df['date_gmt']).apply(lambda x: '{year}'.format(year=x.year))\n",
    "yearlydf = df.groupby('Year_gmt').agg({'like_count':'sum','comment_count':'sum'}).reset_index().sort_values('Year_gmt',ascending= True)\n",
    "yearlydf['pct_like'] = yearlydf['like_count'].pct_change() * 100\n",
    "yearlydf['pct_comment'] = yearlydf['comment_count'].pct_change() * 100\n",
    "yearlydf.sort_values('Year_gmt',ascending= False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings:**\n",
    "\n",
    "The yearly trend shows that likes count started growing from 2005 and increased drastically over the year with increase in 984k % change compare to 2005. Furthermore, for Comments started growing from 2002 and increased exponentially over the years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "#plotting totals \n",
    "plt.figure(figsize=(20,6))\n",
    "ax1 = plt.subplot(111)\n",
    "sns.lineplot(data=yearlydf, x='Year_gmt', y='like_count', palette = 'blue', label ='like_count')\n",
    "# total comments\n",
    "sns.lineplot(data=yearlydf, x='Year_gmt', y='comment_count', palette = 'orange',label = 'comment_count')\n",
    "plt.xticks(rotation = 75, horizontalalignment='center', fontweight='light', fontsize=10)\n",
    "plt.title(\"YEAR: like count vs comment count comparison\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtered data before 2002 as comments started growing from year 2002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Filtered data below year 2002\n",
    "yearlydf1 = yearlydf[yearlydf['Year_gmt'] > '2002'].copy()\n",
    "\n",
    "#plotting totals \n",
    "plt.figure(figsize=(20,4))\n",
    "ax1 = plt.subplot(111)\n",
    "sns.lineplot(data=yearlydf1, x='Year_gmt', y='like_count', palette = 'blue', label ='like_count')\n",
    "# total comments\n",
    "sns.lineplot(data=yearlydf1, x='Year_gmt', y='comment_count', palette = 'orange',label = 'comment_count')\n",
    "plt.xticks(rotation = 75, horizontalalignment='center', fontweight='light', fontsize=10)\n",
    "plt.title(\"YEAR: like count vs comment count comparison\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "matplotlib.rc_file_defaults()\n",
    "ax1 = sns.set_style(style=None, rc=None )\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(15,3))\n",
    "sns.barplot(data = yearlydf1, x='Year_gmt', y='comment_count', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "yearlydf2 = yearlydf[yearlydf['Year_gmt'] > '2009'].copy()\n",
    "\n",
    "matplotlib.rc_file_defaults()\n",
    "ax1 = sns.set_style(style=None, rc=None )\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(15,3))\n",
    "sns.barplot(data = yearlydf2, x='Year_gmt', y='like_count', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Findings: \n",
    "It is evident that like counts are maximum for the year 2013, 2014, 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import datetime\n",
    "dfm = df[df.date_gmt.dt.date > datetime.date(2010,12,1)].copy()\n",
    "df2 = dfm.groupby(dfm.date_gmt.dt.date).agg({'like_count':'sum','comment_count':'sum'}).reset_index()\n",
    "#plotting totals \n",
    "plt.figure(figsize=(22,8))\n",
    "ax1 = plt.subplot(111)\n",
    "sns.lineplot(data=df2, x='date_gmt', y='like_count', palette = 'blue', label ='Daily like_count')\n",
    "plt.xticks(rotation = 75, horizontalalignment='center', fontweight='light', fontsize=10)\n",
    "plt.title(\"Daily: total likes\", fontsize=20)\n",
    "plt.legend(loc='upper right')\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings:\n",
    "It looks like a sudden spike in the daily like counts in the end of 2014\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "df['YearMonth'] = pd.to_datetime(df['date_gmt']).apply(lambda x: '{year}-{month}'.format(year=x.year,month=x.month))\n",
    "monthdf = df.groupby('YearMonth').agg({'like_count':'sum','comment_count':'sum'}).reset_index().sort_values('YearMonth',ascending= True)\n",
    "monthdf['pct_like'] = monthdf['like_count'].pct_change() * 100\n",
    "monthdf['pct_comment'] = monthdf['comment_count'].pct_change() * 100\n",
    "monthdf.sort_values('YearMonth',ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "monthdf1 = monthdf[monthdf['YearMonth'] > '2006-01'].copy()\n",
    "monthdf1.sort_values('YearMonth',inplace=True)\n",
    "monthdf1.set_index('YearMonth')['comment_count'].plot(kind='bar',figsize=(30, 10),color='cadetblue',rot=90, fontsize=12)\n",
    "plt.title(\"Total Monthly comment count\", y=1.013, fontsize=20)\n",
    "plt.ylabel(\"Sum [comment count]\", labelpad=10)\n",
    "plt.xlabel(\"Date [Month - Year]\", labelpad=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings:\n",
    "It shows the spike in the comments counts for the last 3 months of the year end such as September, October, November, December"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: How would you determine which authors are the most “popular”? What additional data would you need?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First step to check if there are any null authors present in the analysis and understand the total cohort size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# check nulls in the authors count\n",
    "print(f'Total Authors: {df[\"author_id\"].count()}')\n",
    "print(f'Null Authors: {df[\"author_id\"].isnull().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand the popularity of any author first steps need to do is that to aggregate data based on the below given key dimension:\n",
    "- Count of Post_id - To understand how many post author has done\n",
    "- Count of blog_id - To understand how many blogs author has publishes\n",
    "- Sum of total comment count - Provides good measure of popularity as reader has shown interest in the authors post and commented on it\n",
    "- Sum of total likes count - provides insights on liking or unliking of the posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "authdf =  df.groupby([\"author_id\",\"author\"]).agg({\"post_id\":'count',\"blog_id\":'count',\"comment_count\":'sum',\"like_count\":'sum'}).reset_index()\n",
    "authdf.sort_values(by = ['comment_count',\"like_count\",'post_id'],ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 authors based on the Number of posts \n",
    "- Found out that author **\"Eli Nathanael\"** has made 42760 posts which is 2.3% of total posts but did not receive much engagement from users after looking at the number of comments and likes \n",
    "- second highest post has been made by author **FLYNN** with post 42760 which accounts for 1.2% of total and received good support and engagement from users in terms of the number of comments and likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "# Top 10 authors based on the Number of posts\n",
    "authdf[\"pct_total_post\"] = (authdf[\"post_id\"] / authdf[\"post_id\"].sum()) * 100\n",
    "authdf.sort_values(by = ['post_id'],ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popularity based on the Engagement metrics:\n",
    "To understand further the popularity of the author we need to look at 2 key metrics which are total comments made on the post which shows engagement from users and other important metric is total likes received which shows the \n",
    "\n",
    "**Likes**: Liking a post is easy. Liking takes only one click. when someone likes your post, you don’t have the opportunity to engage back.\n",
    "\n",
    "**Comments**: This allows a user to type a response to the post.\n",
    "\n",
    "As with likes, the more comments a post receives, the more people will see the post. However, comments have more weight than likes in social media algorithms, so **five comments are worth more than five likes**. Comments are great because they give the opportunity for you to continue the conversation. When a user comments, you can like their comment and reply to it. When you comment on other pages, you stand out from the crowded likes list and many pages will reply to you.\n",
    "\n",
    "**Derived Metric:**\n",
    "- Total Comments received\n",
    "- **Average comments per post**: This shows the average number of comments done on a post "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "authdf[\"Avg_comments_per_post\"] = authdf[\"comment_count\"] / authdf[\"post_id\"]\n",
    "authdf.sort_values(by = ['Avg_comments_per_post'],ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings**\n",
    "\n",
    "After looking at total data for total comment per post found out that author **tonyedwards** have made 162 posts in his total duration on the app and **received 132k comments** which means **received highest ~816 comments per post** but did not recieve any likes which is interesting. There can be multiple reasons and to understand better we need to check yearly comments data to see if likes feature was available at that time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "commentdf = authdf.sort_values(by = ['Avg_comments_per_post'],ascending=False).head(20)\n",
    "\n",
    "matplotlib.rc_file_defaults()\n",
    "ax1 = sns.set_style(style=None, rc=None )\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8,6))\n",
    "sns.barplot(data = commentdf, x='Avg_comments_per_post', y='author', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popularity based on Likes\n",
    "\n",
    "**Derived Metric:**\n",
    "- Total likes received\n",
    "- **Total likes per post**: This shows the average number of likes recieved on a post "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "authdf[\"avg_likes_per_post\"] = authdf[\"like_count\"] / authdf[\"post_id\"]\n",
    "authdf.sort_values(by = ['avg_likes_per_post'],ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings**\n",
    "\n",
    "After looking at total data for Average likes per post found out that author **Nicole Marie** has made only 1 post in his total duration on the app and received 241 likes which means **received highest ~241 likes per post** and also recieved 78 comments shows that high engagement from users on that one post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "likesdf = authdf.sort_values(by = ['avg_likes_per_post'],ascending=False).head(20)\n",
    "\n",
    "matplotlib.rc_file_defaults()\n",
    "ax1 = sns.set_style(style=None, rc=None )\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8,6))\n",
    "sns.barplot(data = likesdf, x='avg_likes_per_post', y='author', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popularity based on Yearly trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "authdf1 =  df.groupby([\"author_id\",'author','Year_gmt']).agg({\"post_id\":'count',\"blog_id\":'count',\"comment_count\":'sum',\"like_count\":'sum'}).reset_index()\n",
    "# Creating metrics to better understand engagement \n",
    "authdf1[\"avg_likes_per_post\"] = authdf1[\"like_count\"] / authdf1[\"post_id\"]\n",
    "authdf1[\"Avg_comments_per_post\"] = authdf1[\"comment_count\"] / authdf1[\"post_id\"]\n",
    "authdf1.sort_values(by = ['comment_count',\"like_count\"],ascending=False).head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "authdf1 = authdf1[authdf1['Year_gmt'] > '2002'].copy()\n",
    "authdf2 = authdf1.sort_values(['Year_gmt','comment_count'], ascending=False).groupby('Year_gmt').head(1).copy()\n",
    "authdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "matplotlib.rc_file_defaults()\n",
    "ax1 = sns.set_style(style=None, rc=None )\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8,6))\n",
    "sns.barplot(data = authdf2, x='comment_count', y='author', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings\n",
    "\n",
    "After looking at total data for Average comments count per post found out that author Tonyedwards has more than 120000 comments counts in his total duration on the app. Seconds highest comments counts receiver is Ron DuBour who received around 60000 comments counts which can be interpreted as high engagement from these two authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What additional data would you need?¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The key missing feature in this data is the **actual comments given by users**. it would have helped use to perform **sentimental analysis** to understand the emotions and liking for each post. \n",
    "\n",
    "**User acquisition Funnel analysis**\n",
    "- **First post/Blog date or Author signup date**: signup or first post date would have helped us to create retention analyis to understand the health of the user acquision funnel\n",
    "- Missing details about users and their signup to first comment/like dates or flag\n",
    "\n",
    "**Demographic data**\n",
    "- Missing demographic data like age, gender, education would have helped use to better segment data into multiple funnels\n",
    "- Author type: eg, poet, Novelist, writer, Lyricist, screen writer, play write\n",
    "- Post/Blog type: eg.music, sport, reviews, news, updates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a unique insight you can provide us about the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Metrics yearly trend analysis:\n",
    "- Total Active authors per year\n",
    "- Total Posts per year\n",
    "- Total Comments received\n",
    "- Average comments per post per year: This shows the average number of comments done on a post \n",
    "- Total Likes received\n",
    "- Average likes per post per year : This shows the average number of comments done on a post "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "insdf =  df.groupby(['Year_gmt']).agg({'author_id':'count','blog_id':'count','post_id':'count','comment_count':'sum','like_count':'sum'}).reset_index()\n",
    "\n",
    "# Creating metrics to better understand engagement\n",
    "insdf[\"avg_likes_per_post\"] = (insdf[\"like_count\"] / insdf[\"post_id\"]) * 100\n",
    "insdf[\"Avg_comments_per_post\"] = (insdf[\"comment_count\"] / insdf[\"post_id\"]) * 100\n",
    "insdf[\"Avg_comments_per_author\"] = (insdf[\"comment_count\"] / insdf[\"author_id\"]) * 100\n",
    "insdf.sort_values(by = ['Year_gmt'],ascending=False).head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights:**\n",
    "    \n",
    "- **Yearly Active Authors + New user Analysis**: The data shows that active Author counts doubled every year with growth rate of ~200% starting from 2007 and reached till ~450% in year 2011 and 2012 and reached to highest in 2014 which was 619470\n",
    "- **User Engagement**: Key metrics to measure engagement is number comments and likes on the post and allows users to interact with Authors which keeps platform alive. So, decided to measure engagement using to 2 key metrics as follows:\n",
    "- avg comments per post in a year & avg like per post in a year:\n",
    "It shows that starting from 2010 comments per post count increased drastically with avg. count of ~300 comments per posts \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "fig, axes = plt.subplots(3, 2, sharex=True, figsize=(15,60))\n",
    "fig.suptitle(f\"Key Metrics Analysis: Authors\",fontsize=30)\n",
    "insdf1 = insdf[insdf['Year_gmt'] > '2005-01'].copy()\n",
    "insdf1.sort_values('Year_gmt',inplace=True)\n",
    "# Author count\n",
    "insdf1.set_index('Year_gmt')['author_id'].plot(ax=axes[0, 0],kind='bar',figsize=(30, 10),color='cadetblue',rot=90, fontsize=12)\n",
    "axes[0,0].set_title(f\"Total Yearly Author Count\",fontsize=20)\n",
    "plt.title(\"\", y=1.013, fontsize=20)\n",
    "axes[0,0].set_ylabel(\"Unique Author count]\", labelpad=10)\n",
    "axes[0,0].set_xlabel(\"Date [Year]\", labelpad=10)\n",
    "\n",
    "# post Count\n",
    "insdf1.set_index('Year_gmt')['post_id'].plot(ax=axes[0, 1],kind='bar',figsize=(30, 10),color='cadetblue',rot=90, fontsize=12)\n",
    "axes[0,1].set_title(f\"Total Yearly blog Count\",fontsize=20)\n",
    "plt.title(\"\", y=1.013, fontsize=20)\n",
    "axes[0,1].set_ylabel(\"Sum [post count]\", labelpad=10)\n",
    "axes[0,1].set_xlabel(\"Date [Year]\", labelpad=10)\n",
    "\n",
    "# Total Yearly Comment Count\n",
    "insdf1.set_index('Year_gmt')['comment_count'].plot(ax=axes[1, 0],kind='bar',figsize=(30, 10),color='cadetblue',rot=90, fontsize=12)\n",
    "axes[1,0].set_title(f\"Total Yearly Comment Count\",fontsize=20)\n",
    "plt.title(\"\", y=1.013, fontsize=20)\n",
    "axes[1,0].set_ylabel(\"Sum [comment count]\", labelpad=10)\n",
    "axes[1,0].set_xlabel(\"Date [Year]\", labelpad=10)\n",
    "\n",
    "# Total Yearly like Count\n",
    "insdf1.set_index('Year_gmt')['like_count'].plot(ax=axes[1, 1],kind='bar',figsize=(30, 10),color='cadetblue',rot=90, fontsize=12)\n",
    "axes[1,1].set_title(f\"Total Yearly likes Count\",fontsize=20)\n",
    "plt.title(\"\", y=1.013, fontsize=20)\n",
    "axes[1,1].set_ylabel(\"Sum [likes count]\", labelpad=10)\n",
    "axes[1,1].set_xlabel(\"Date [Year]\", labelpad=10)\n",
    "\n",
    "# Avg_comments_per_post\n",
    "insdf1.set_index('Year_gmt')['Avg_comments_per_post'].plot(ax=axes[2, 0],kind='bar',figsize=(30, 10),color='cadetblue',rot=90, fontsize=12)\n",
    "axes[2,0].set_title(f\"Avg_comments_per_post\",fontsize=20)\n",
    "plt.title(\"\", y=1.013, fontsize=20)\n",
    "axes[2,0].set_ylabel(\"Avg_comments_per_post\", labelpad=10)\n",
    "axes[2,0].set_xlabel(\"Date [Year]\", labelpad=10)\n",
    "\n",
    "# avg_likes_per_post\n",
    "insdf1.set_index('Year_gmt')['avg_likes_per_post'].plot(ax=axes[2, 1],kind='bar',figsize=(30, 10),color='cadetblue',rot=90, fontsize=12)\n",
    "axes[2,1].set_title(f\"Avg_likes_per_post\",fontsize=20)\n",
    "plt.title(\"Avg_likes_per_post\", y=1.013, fontsize=20)\n",
    "axes[2,1].set_ylabel(\"avg_likes_per_post\", labelpad=10)\n",
    "axes[2,1].set_xlabel(\"Date [Year]\", labelpad=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings:\n",
    "\n",
    "1. Total yearly author count: Total yearly author count is highest in the year 2014\n",
    "2. Total yearly blog count: Total yearly blog count is highest in the year 2014\n",
    "3. Total yearly comment count: Total yearly comment count is highest in the year 2014\n",
    "4. Total yearly likes count: Total yearly author count is highest in the year 2014 \n",
    "    and there were no likes on the post prior to 2011\n",
    "5. Average comment per post: Average comment per post has been significantly high since 2006. \n",
    "    With highest comment counts in year 2011 and 2012\n",
    "6. Average likes per post: Average likes per post has been huge since 2013 to 2015. \n",
    "    Likes has become stifer on the post since 2010 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Creating metrics to better understand engagement \n",
    "authdf1[\"avg_likes_per_post\"] = authdf1[\"like_count\"] / authdf1[\"post_id\"]\n",
    "authdf1[\"Avg_comments_per_post\"] = authdf1[\"comment_count\"] / authdf1[\"post_id\"]\n",
    "authdf1.sort_values(by = ['comment_count',\"like_count\"],ascending=False).head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Blogs by Author analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "blogdf = df.groupby([\"author_id\",'author']).agg({\"blog_id\":\"count\"}).reset_index().sort_values(\"blog_id\",ascending=False).head(20)\n",
    "\n",
    "blogdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "matplotlib.rc_file_defaults()\n",
    "ax1 = sns.set_style(style=None, rc=None )\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8,6))\n",
    "sns.barplot(data = blogdf, x='blog_id', y='author', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings:\n",
    "\n",
    "1. Author named as 'Eli nathnael' has written a highest numbers of blogs\n",
    "2. Author named as 'walter' has written a lowest numbers of blogs in the top 20 writers as per the count of written blogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key metrics by language analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "langdf =  df.groupby(['lang']).agg({'author_id':'count','blog_id':'count','post_id':'count','comment_count':'sum','like_count':'sum'}).reset_index()\n",
    "# Creating metrics to better understand engagement\n",
    "langdf[\"avg_likes_per_post\"] = (langdf[\"like_count\"] / langdf[\"post_id\"]) * 100\n",
    "langdf[\"Avg_comments_per_post\"] = (langdf[\"comment_count\"] / langdf[\"post_id\"]) * 100\n",
    "langdf[\"Avg_comments_per_author\"] = (langdf[\"comment_count\"] / langdf[\"author_id\"]) * 100\n",
    "langdf[\"pct_total_authors\"] = (langdf[\"author_id\"]/langdf[\"author_id\"].sum()) * 100\n",
    "langdf.sort_values(by = [\"comment_count\"],ascending=False).head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings**\n",
    "\n",
    "Found out that **92% of authors have written blogs/posts in the \"English\" language** and received the highest number of comments and likes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "fig, axes = plt.subplots(3, 2, sharex=True, figsize=(15,50))\n",
    "fig.suptitle(f\"Key Metrics Analysis by Language\",fontsize=30)\n",
    "# langdf = insdf[insdf['lang'] > '2005-01'].copy()\n",
    "langdf = langdf.sort_values(by = [\"comment_count\"],ascending=False).head(10).copy()\n",
    "# Author count\n",
    "langdf.set_index('lang')['author_id'].plot(ax=axes[0, 0],kind='bar',figsize=(30, 10),color='cadetblue',rot=90, fontsize=12)\n",
    "axes[0,0].set_title(f\"Total Yearly Author Count\",fontsize=20)\n",
    "plt.title(\"\", y=1.013, fontsize=20)\n",
    "axes[0,0].set_ylabel(\"Unique Author count]\", labelpad=10)\n",
    "axes[0,0].set_xlabel(\"Language\", labelpad=10)\n",
    "\n",
    "# post Count\n",
    "langdf.set_index('lang')['post_id'].plot(ax=axes[0, 1],kind='bar',figsize=(30, 10),color='cadetblue',rot=90, fontsize=12)\n",
    "axes[0,1].set_title(f\"Total Yearly blog Count\",fontsize=20)\n",
    "plt.title(\"\", y=1.013, fontsize=20)\n",
    "axes[0,1].set_ylabel(\"Sum [post count]\", labelpad=10)\n",
    "axes[0,1].set_xlabel(\"Language\", labelpad=10)\n",
    "\n",
    "# Total Yearly Comment Count\n",
    "langdf.set_index('lang')['comment_count'].plot(ax=axes[1, 0],kind='bar',figsize=(30, 10),color='cadetblue',rot=90, fontsize=12)\n",
    "axes[1,0].set_title(f\"Total Yearly Comment Count\",fontsize=20)\n",
    "plt.title(\"\", y=1.013, fontsize=20)\n",
    "axes[1,0].set_ylabel(\"Sum [comment count]\", labelpad=10)\n",
    "axes[1,0].set_xlabel(\"Language\", labelpad=10)\n",
    "\n",
    "# Total Yearly like Count\n",
    "langdf.set_index('lang')['like_count'].plot(ax=axes[1, 1],kind='bar',figsize=(30, 10),color='cadetblue',rot=90, fontsize=12)\n",
    "axes[1,1].set_title(f\"Total Yearly likes Count\",fontsize=20)\n",
    "plt.title(\"\", y=1.013, fontsize=20)\n",
    "axes[1,1].set_ylabel(\"Sum [likes count]\", labelpad=10)\n",
    "axes[1,1].set_xlabel(\"Language\", labelpad=10)\n",
    "\n",
    "# Avg_comments_per_post\n",
    "langdf.set_index('lang')['Avg_comments_per_post'].plot(ax=axes[2, 0],kind='bar',figsize=(30, 10),color='cadetblue',rot=90, fontsize=12)\n",
    "axes[2,0].set_title(f\"Avg_comments_per_post\",fontsize=20)\n",
    "plt.title(\"\", y=1.013, fontsize=20)\n",
    "axes[2,0].set_ylabel(\"Avg_comments_per_post\", labelpad=10)\n",
    "axes[2,0].set_xlabel(\"Language\", labelpad=10)\n",
    "\n",
    "# avg_likes_per_post\n",
    "langdf.set_index('lang')['avg_likes_per_post'].plot(ax=axes[2, 1],kind='bar',figsize=(30, 10),color='cadetblue',rot=90, fontsize=12)\n",
    "axes[2,1].set_title(f\"Avg_likes_per_post\",fontsize=20)\n",
    "plt.title(\"Avg_likes_per_post\", y=1.013, fontsize=20)\n",
    "axes[2,1].set_ylabel(\"avg_likes_per_post\", labelpad=10)\n",
    "axes[2,1].set_xlabel(\"Language\", labelpad=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings:\n",
    "\n",
    "1. Total yearly author count: Total yearly author count is highest in the language 'English'\n",
    "2. Total yearly blog count: Total yearly blog count is highest in the language 'English'\n",
    "3. Total yearly comment count: Total yearly comment count is highest in the language 'English'\n",
    "4. Total yearly likes count: Total yearly author count is highest in the language 'English'\n",
    "5. Average comment per post: Average comment per post is surprisingly low in language 'English' \n",
    "    and high in language 'tl' and language 'fr'\n",
    "6. Average likes per post: Average likes per post has been normally distributed for the language other than 'English'.\n",
    "For language 'English', Average likes per post are highest\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool used for this analysis are as follows:\n",
    "Json\n",
    "Gzip - to unzip file from json to csv\n",
    "import os import json import gzip\n",
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "## Dataframe and Series\n",
    "\n",
    "import pandas as pd import numpy as np from datetime import datetime, timedelta, date from datetime import *\n",
    "\n",
    "import matplotlib.pyplot as plt import seaborn as sns import plotly.express as px import matplotlib\n",
    "\n",
    "import warnings warnings.filterwarnings('ignore')\n",
    "\n",
    "import warnings warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "plt.style.use('fivethirtyeight') %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dicts[\"sdf\"]= dict(14)\n",
    "\n",
    "\n",
    "dicts = { 'sdf': {'20': '120'},\n",
    "                'himani': {'22': '50'},\n",
    "               'Rahul': {'22': '70'},\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dicts[\"vela\"]= [14,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sdf': {'20': '120'}, 'himani': {'22': '50'}, 'Rahul': {'22': '70'}}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdf {'20': '120'}\n",
      "himani {'22': '50'}\n",
      "Rahul {'22': '70'}\n"
     ]
    }
   ],
   "source": [
    "for key, values in dicts.items():\n",
    "    print(key,values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('car', 'abc'), ('score', '15'), ('rental_rate', '120')])\n",
      "dict_items([('car', 'xyz'), ('score', '20'), ('rental_rate', '50')])\n",
      "dict_items([('car', 'pqr'), ('score', '20'), ('rental_rate', '70')])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "val=[]\n",
    "for i, values in enumerate(dicts.values()):\n",
    "    print(values.items())\n",
    "#     val.append(values[0])\n",
    "# print(max(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rental_rate</th>\n",
       "      <td>120</td>\n",
       "      <td>50</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               1   2   3\n",
       "score         15  20  20\n",
       "rental_rate  120  50  70"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(dicts) # convert dictionary to dataframe\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-bf665d6180d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# df.sort(by = [score, rantal_rate], ascending=[False, True])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# (axis=1).nlargest(2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rental_rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36msort_values\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   5440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5442\u001b[0;31m             \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5444\u001b[0m             \u001b[0;31m# need to rewrap columns in Series to apply key function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   5440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5442\u001b[0;31m             \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5444\u001b[0m             \u001b[0;31m# need to rewrap columns in Series to apply key function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1682\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1684\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'score'"
     ]
    }
   ],
   "source": [
    "# df.sort(by = [score, rantal_rate], ascending=[False, True])  \n",
    "# (axis=1).nlargest(2)\n",
    "df.sort_values(by=['score', 'rental_rate'], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = { '1': {\"car\": 'abc', \"score\": '15', 'rental_rate':'120'},\n",
    "         '2': {\"car\": 'xyz',\"score\": '20', 'rental_rate':'50'},\n",
    "          '3': {\"car\": 'pqr', \"score\": '20', 'rental_rate':'70'}\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-81-b1a5e0ef6f3d>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-81-b1a5e0ef6f3d>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    {\"car\": 'xyz',\"score\": '20', 'rental_rate':'50'},\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "dicts = {\"car\": 'abc', \"score\": '15', 'rental_rate':'120'},\n",
    "         {\"car\": 'xyz',\"score\": '20', 'rental_rate':'50'},\n",
    "          {\"car\": 'pqr', \"score\": '20', 'rental_rate':'70'}\n",
    "\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = { '1': { \"score\": '15', 'rental_rate':'120'},\n",
    "         '2': {\"score\": '20', 'rental_rate':'50'},\n",
    "          '3': { \"score\": '20', 'rental_rate':'70'}\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
